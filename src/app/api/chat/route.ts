import { NextRequest, NextResponse } from 'next/server'

const SYSTEM_PROMPT = `ë‹¹ì‹ ì€ Makeitì˜ AI ì–´ì‹œìŠ¤í„´íŠ¸ìž…ë‹ˆë‹¤. ì‚¬ìš©ìžê°€ ì›í•˜ëŠ” ì›¹ ì•±ì„ ì„¤ëª…í•˜ë©´, ìš”êµ¬ì‚¬í•­ì„ ëª…í™•ížˆ íŒŒì•…í•˜ê³  êµ¬í˜„ ê³„íšì„ ì„¸ì›ë‹ˆë‹¤.

ì—­í• :
1. ì‚¬ìš©ìžì˜ ì•± ì•„ì´ë””ì–´ë¥¼ ì´í•´í•˜ê³  êµ¬ì²´í™”
2. í•„ìš”í•œ ê¸°ëŠ¥, íŽ˜ì´ì§€, ë°ì´í„° êµ¬ì¡° ì •ë¦¬
3. ê¸°ìˆ  ìŠ¤íƒ ì œì•ˆ (Next.js 14 + Tailwind CSS + TypeScript ê¸°ë³¸)
4. êµ¬í˜„ ê°€ëŠ¥í•œ MVP ë²”ìœ„ í™•ì •

ì‘ë‹µ ìŠ¤íƒ€ì¼:
- ì¹œê·¼í•˜ê³  ê°„ê²°í•˜ê²Œ
- ì´ëª¨ì§€ ì ì ˆížˆ ì‚¬ìš©
- í•œêµ­ì–´ë¡œ ì‘ë‹µ
- ê¸°ìˆ ì  ì„¸ë¶€ì‚¬í•­ì€ í•„ìš”í•  ë•Œë§Œ

ì‚¬ìš©ìžê°€ ì•± ì•„ì´ë””ì–´ë¥¼ ì¶©ë¶„ížˆ ì„¤ëª…í•˜ë©´, ë§ˆì§€ë§‰ì— ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ìš”ì•½:

---
ðŸ“‹ **í”„ë¡œì íŠ¸ ìš”ì•½**
- ì•± ì´ë¦„: [ì œì•ˆ]
- ì£¼ìš” ê¸°ëŠ¥: [ë¦¬ìŠ¤íŠ¸]
- íŽ˜ì´ì§€ êµ¬ì„±: [ë¦¬ìŠ¤íŠ¸]
- ì˜ˆìƒ íŒŒì¼: [ê°¯ìˆ˜]

ì¤€ë¹„ë˜ë©´ "ìƒì„±í•˜ê¸°" ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”!
---`

async function chatWithOllama(messages: { role: string; content: string }[]) {
  const ollamaUrl = process.env.OLLAMA_BASE_URL || 'http://localhost:11434'
  const model = process.env.OLLAMA_MODEL || 'llama3.2'
  
  const response = await fetch(`${ollamaUrl}/api/chat`, {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({
      model,
      messages: [
        { role: 'system', content: SYSTEM_PROMPT },
        ...messages.map(m => ({ role: m.role, content: m.content }))
      ],
      stream: false,
    }),
  })

  if (!response.ok) {
    throw new Error(`Ollama error: ${response.status}`)
  }

  const data = await response.json()
  return data.message?.content || ''
}

async function chatWithAnthropic(messages: { role: string; content: string }[]) {
  const Anthropic = (await import('@anthropic-ai/sdk')).default
  const anthropic = new Anthropic({
    apiKey: process.env.ANTHROPIC_API_KEY,
  })

  const response = await anthropic.messages.create({
    model: 'claude-3-5-sonnet-20241022',
    max_tokens: 1024,
    system: SYSTEM_PROMPT,
    messages: messages.map(m => ({
      role: m.role as 'user' | 'assistant',
      content: m.content,
    })),
  })

  const content = response.content[0]
  return content.type === 'text' ? content.text : ''
}

export async function POST(request: NextRequest) {
  try {
    const { messages } = await request.json()

    let text: string

    // Try Ollama first if configured, otherwise use Anthropic
    if (process.env.OLLAMA_BASE_URL || !process.env.ANTHROPIC_API_KEY) {
      try {
        text = await chatWithOllama(messages)
      } catch (ollamaError) {
        // Fallback to Anthropic if Ollama fails and key exists
        if (process.env.ANTHROPIC_API_KEY) {
          text = await chatWithAnthropic(messages)
        } else {
          throw ollamaError
        }
      }
    } else {
      text = await chatWithAnthropic(messages)
    }

    return NextResponse.json({ message: text })
  } catch (error) {
    console.error('Chat API error:', error)
    const errorMessage = error instanceof Error ? error.message : 'Unknown error'
    return NextResponse.json(
      { error: `Failed to generate response: ${errorMessage}` },
      { status: 500 }
    )
  }
}
